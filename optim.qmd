# 최적화 방법

## Optimization

통계에서 최적화를 해야 하는 상황

- Maximum likelihood $\max_{\theta} L(\theta | \pmb{y})$

## Maximum Likelihood Theory

- For independent data: **likelihood function**
$$
L(\theta) = \prod_{i=1}^n f(\pmb{x}_i | \theta)
$$

- **Log-likelihood function**
$$
\ell (\theta) = \sum_{i=1}^n \log (f(\pmb{x}_i ; \theta))
$$

- Maximum likelihood estimate: $\hat{\theta}_{\text{ML}}=\text{argmax}_{\theta} L(\theta)$

- For smooth likelihoods, necessary requirement:

\begin{align*}
\pmb{s}(\pmb{\theta}) &\equiv \ell ' (\pmb{\theta}), \quad{} |\pmb{\theta}| \text{ equations, called score vector}\\
\pmb{J}(\pmb{\theta}) &\equiv -\ell '' (\pmb{\theta}), \quad{}\text{ positive (definite), called observed Fisher information}
\end{align*}

- Theory:
  + $E[\pmb{s}(\pmb{\theta})] = 0$
  + $\pmb{I}(\pmb{\theta}) \equiv - E[\ell '' (\pmb{\theta})] = E[\pmb{J}(\pmb{\theta})]= \text{Var}[\pmb{s}(\pmb{\theta})], \quad{} \text{expected Fisher information}$
  + For large $n$ (and some regularity assumptions)
  $$
  \hat{\pmb{\theta}}_{\text{Ml}} \approx \mathcal{N}(\pmb{\theta}, \pmb{I}^{-1}(\hat{\pmb{\theta}}_{\text{ML}})) \approx \mathcal{N}(\pmb{\theta}, \pmb{J}^{-1}(\hat{\pmb{\theta}}_{\text{ML}}))
  $$
  
## Newton의 방법

**Q**. 뉴턴법은 언제 쓰는가?

- $f(x)=0$의 해를 근사적으로 구할 때

- $g(x)=h(x)$인 $x$를 근사적으로 구할 때

- $f(x)$의 최소값 또는 최대값을 구할 때

**Q**. 뉴턴법의 한계

- 해가 여러개 일 경우

- 수렴속도가 초깃값에 따라 달라짐

**Q**. 통계에서의 뉴턴법? ML estimation

1차원에서의 ML 추정을 생각해보자.
$$
\text{argmax}_{\theta} L(\theta | \pmb{y}) = \text{argmax}_{\pmb{\theta}}\underbrace{\log L(\theta | \pmb{y})}_{\ell (\theta | \pmb{y})}
$$

이것을 $\theta^{*}$ 근처에서 테일러 근사로 전개해보자.

\begin{align*}
\ell (\theta) &\approx \ell (\theta^{*}) + (\theta - \theta^{*}) \ell ' (\theta^{*}) + \frac{1}{2}(\theta - \theta^{*})^2 \ell '' (\theta^{*})\\
&= \ell (\theta^{*}) + (\theta - \theta^{*}) s (\theta^{*}) - \frac{1}{2}(\theta - \theta^{*})^2 J (\theta^{*})
\end{align*}

즉 score 함수는 $s(\theta) = \ell ' (\theta)$, observed information은 $J(\theta) = - \ell '' (\theta)$이다.

- Solving the maximum of the approximation
$$
\theta = \theta^{*} + \frac{s(\theta^{*})}{J(\theta^{*})} = \theta^{*} - \frac{\ell ' (\theta^{*})}{\ell '' (\theta^{*})}
$$

### Stoping criteria

- 반복법에서는 어느 시점에 어떤 기준을 가지고 멈춰야 할지를 정하는 것이 중요하다.

1. Absolute convergence: 이는 $x$가 클 경우에 시간이 오래 걸릴 수 있다.
$$|x^{(t+1)} -x^{(t)}| < \varepsilon \text{ or }\| \pmb{x}^{(t+1)} - \pmb{x}^{(t)} \| < \varepsilon$$

2. Relative convergence: 이는 $|x^{(t)}|$가 작을 경우에 불안정할 수 있다.
$$
\frac{|x^{(t+1)}- x^{(t)}|}{|x^{(t)}|} < \varepsilon \text{ or }\frac{\|x^{(t+1)}- x^{(t)}\|}{\|x^{(t)}\|} < \varepsilon
$$

3. After $N$ iterations (additional criteria)

### Log likelihood의 score와 observed information

- Likelihood fct ($\sigma$ is known)
$$
L(\mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp \Big\{-\frac{1}{2}\Big( \frac{x_i- \mu}{\sigma} \Big)^2 \Big\}
$$

- Log-likelihood
$$
\ell(\mu) = \sum_{i=1}^n -\frac{1}{2}\log 2\pi - \frac{1}{2}\log \sigma^2 - \frac{1}{2}\Big( \frac{x_i -\mu}{\sigma} \Big)^2
$$

- Score fct
$$
s(\mu) = \ell ' (\mu) = \sum_{i=1}^n -0 - 0 - \frac{x_i - \mu}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^n  (x_i - \mu)
$$

- Information 
$$
J(\mu) = -\ell '' (\mu) = - s' (\mu) = - \frac{1}{\sigma^2}\sum_{i=1}^n (-1) = \frac{n}{\sigma^2}
$$

### Multidimensional extension

- Consider log likelihood:
$$
\text{argmax}_{\pmb{\theta}} \ell (\pmb{\theta}) = \text{argmax}_{\pmb{\theta}} L(\pmb{\theta}|\pmb{y})= \text{argmax}_{\pmb{\theta}} \log( L(\pmb{\theta}|\pmb{y}) )
$$

- 그러면 다차원에서 테일러 근사는 다음과 같다.

\begin{align*}
\ell (\pmb{\theta}) &\approx \ell (\pmb{\theta}^{*}) + (\pmb{\theta} - \pmb{\theta}^{*})^T \ell ' (\pmb{\theta}^{*}) + \frac{1}{2} (\pmb{\theta} - \pmb{\theta}^{*})^T \pmb{H}(\pmb{\theta}^{*})(\pmb{\theta}-\pmb{\theta}^{*})\\
&\approx \ell (\pmb{\theta}^{*}) + (\pmb{\theta} - \pmb{\theta}^{*})^T \pmb{s} (\pmb{\theta}^{*}) - \frac{1}{2} (\pmb{\theta} - \pmb{\theta}^{*})^T \pmb{J}(\pmb{\theta}^{*})(\pmb{\theta}-\pmb{\theta}^{*})
\end{align*}

- Score function:
$$
\pmb{s} (\pmb{\theta}) = \nabla \ell (\pmb{\theta}) = \frac{\partial}{\partial \pmb{\theta}} \ell (\pmb{\theta})
$$

- Observed information:
$$
\pmb{J} (\pmb{\theta}) = -\nabla^2 \ell (\pmb{\theta}) = \frac{\partial^2}{\partial \pmb{\theta}^2}\ell (\pmb{\theta})
$$

- 여기서는 반복법을 이용해 다음의 근사를 풀어 최대화한다.
$$
\pmb{\theta} = \pmb{\theta}^{*} + \pmb{J}(\pmb{\theta}^{*})^{-1}\pmb{s}(\pmb{\theta}^{*}) = \pmb{\theta}^{*} - \pmb{H}(\pmb{\theta}^{*})^{-1}\nabla \ell (\pmb{\theta}^{*})
$$

### $\mathbb{R}^p$에서의 log likelihood의 score와 observed information

- Likelihood: $\Sigma$가 알려져 있다고 할 때
$$
L(\mu) = \prod_{i=1}^n \frac{1}{\sqrt{(2\pi)^p |\Sigma|}}\exp \Big\{ - \frac{1}{2}(x_i - \mu)^T \Sigma^{-1}(x_i - \mu) \Big\}
$$

- Log-likelihood
$$
\ell (\mu) = \sum_{i=1}^n -\frac{p}{2}\log 2\pi - \frac{1}{2}\log |\Sigma | - \frac{1}{2} (x_i - \mu)^T \Sigma^{-1}(x_i - \mu)
$$

- Score function
$$
s(\mu) = \nabla \ell (\mu) = \sum_{i=1}^n \Sigma^{-1} (x_i - \mu) = \Sigma^{-1}\sum_{i=1}^n (x_i - \mu)
$$

- Information
$$
J(\mu) = - \nabla^2 \ell (\mu) = - \sum_{i=1}^n -\Sigma^{-1} = n \Sigma^{-1} = \Big( \frac{1}{n}\Sigma \Big)^{-1}
$$

## Fisher scoring

- 뉴턴의 방법은 $\ell'' (\theta) <0$이거나 $J(\theta) >0$이라는 조건을 필요로 한다. 다변량에서는 $\pmb{J}(\pmb{\theta})$가 positive definite여야 한다.

- $\pmb{J}(\pmb{\theta})$는 stochastic (depend on data)

- $\pmb{I}(\pmb{\theta}) = E[ \pmb{J}(\pmb{\theta})]$: **expected information matrix**

- 이때 $\pmb{I}(\pmb{\theta}) = \text{Var}[\pmb{s}(\pmb{\theta})]$가 항상 positive (semi-)definite임을 보일 수 있다.

**Fisher scoring algorithm**은 다음과 같다.
$$
\pmb{\theta}^{(t+1)} = \pmb{\theta}^{(t)} + [\pmb{I}(\pmb{\theta}^{(t)})]^{-1}\pmb{s}(\pmb{\theta}^{(t)})
$$
이렇게 하면 계산이 좀 더 쉬워지고 뉴턴의 방법에 비해 좀 더 안정적으로 된다고 한다.

::: {#exm-Aitken}
## $p$차원에서 정규분포를 따르는 자료의 $\pmb{I}(\pmb{\theta})$

$\mathbb{R}^p$에서 $I(\mu) = E(J(\mu)) = \text{Var}(s(\mu))$임을 보일 수 있다.

- $s(\mu) = \Sigma^{-1} \sum_{i=1}^n (x_i - \mu)$

- $J(\mu) = \Big( \frac{1}{n}\Sigma \Big)^{-1}$

- $E(J(\mu)) = E\Big( (\frac{1}{n}\Sigma)^{-1} \Big)= (\frac{1}{n|\Sigma)^{-1}$

- $s(\mu)$의 분산

\begin{align*}
\text{Var}(s(\mu)) &= \text{Var} \Big( \Sigma^{-1} \sum_{i=1}^n (x_i - \mu)\Big)\\
&= \sum_{i=1}^n \Sigma^{-1}\text{Var}(x_i - \mu)\Sigma^{-1} \quad{} (\because \text{independence})\\
&= \sum_{i=1}^n \Sigma^{-1}\Sigma \Sigma^{-1} = n \Sigma^{-1}\\
&= \Big( \frac{1}{n}\Sigma \Big)^{-1}
\end{align*}

:::

## Gauss-Newton method

**Q**. 가우스-뉴턴법: 뉴턴법을 연립방정식 형태로 확장시킨 것(??)

- 다음과 같은 모형을 생각해보자.
$$
Y_i = f(\pmb{z}_i ; \pmb{\theta}) + \varepsilon_i
$$
그리고 $g(\pmb{\theta}) = -\sum_{i=1}^n (y_i - f(\pmb{z}_i; \pmb{\theta}))^2$을 최대화하고자 한다.

1. 뉴턴의 방법은 $g(\pmb{\theta})$를 근사하는 방법이다.

2. 가우스-뉴턴의 방법은 $f(\pmb{z}_i;\pmb{\theta})$를 근사하는 방법이다.
$$
\tilde{f}(\pmb{z}_i ; \pmb{\theta}; \pmb{\theta}^{(t)}) \approx f(\pmb{z}_i; \pmb{\theta}^{(t)}) + (\pmb{\theta} - \pmb{\theta}^{(t)})\nabla_{\pmb{\theta}} f(\pmb{z}_i, \pmb{\theta}^{(t)})
$$

- Gauss-Newton step은 다음을 최대화하는 것이다:

\begin{align*}
\tilde{g}(\pmb{\theta}) &= - \sum_{i=1}^n (y_i - \tilde{f}(\pmb{z}_i ; \pmb{\theta}; \pmb{\theta}^{(t)}))^2\\
&= -\sum_{i=1}^n [y_i - f(\pmb{z}_i; \pmb{\theta}^{(t)})+ (\pmb{\theta}-\pmb{\theta}^{(t)})^T\nabla_{\pmb{\theta}}f(\pmb{z}_i, \pmb{\theta}^{(t)})]^2
\end{align*}

- 이것의 해(solution)는 다음과 같다:
$$
\pmb{\theta}^{(t+1)} = \pmb{\theta}^{(t)}+ [(\pmb{A}^{(t)})^T\pmb{A}^{(t)} ]^{-1}(\pmb{A}^{(t)})^T[\pmb{y} - \pmb{f}(\pmb{z}; \pmb{\theta}^{(t)})]
$$
이때
$$
\pmb{f}(\pmb{z}; \pmb{\theta}) = \begin{bmatrix}
f(\pmb{z}_1;\pmb{\theta})\\
\vdots\\
f(\pmb{z}_n;\pmb{\theta})
\end{bmatrix}, \quad{}
\pmb{A}^{(t)}_{n\times p} = \begin{bmatrix}
\nabla_{\theta} f(\pmb{z}_1, \theta)\\
\vdots\\
\nabla_{\theta} f(\pmb{z}_n, \theta)
\end{bmatrix}
$$

- 장점: first derivatives만 있어도 된다.

### 회귀분석에서의 가우스-뉴턴법

- 회귀분석: 다음을 푸는 것
$$
\min_{\beta} \sum_{i=1}^n (y_i - \beta^T x_i)^2
$$

$\pmb{X} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}$이라고 할 때 최소제곱법으로 구하면 다음과 같다.

\begin{align*}
&\min_{\pmb{\beta}}(\pmb{y} - \pmb{X}\pmb{\beta})^T(\pmb{y} - \pmb{X}\pmb{\beta})\\
&\frac{\partial}{\partial\pmb{\beta}} 2\pmb{X}^T(\pmb{y}-\pmb{X\beta})=0\\
&\pmb{\beta} = (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
\end{align*}

- 이것을 근사 방법으로 풀면 다음과 같다.

\begin{align*}
&\min_{\pmb{\beta}}\sum_{i=1}^n (y_i - f(z_i, \theta))^2\\
&\min_{\pmb{\beta}}\sum_{i=1}^n (y_i - f(z_i, \theta_k)- (\theta-\theta_k)^T\nabla f(z_i, \theta_k))^2\\
\theta_{k+1} &= \theta_k + \Big( \pmb{A}^{(k)^T}\pmb{A}^{(k)}  \Big)^{-1}\pmb{A}^{(k)^T}(\pmb{y}-\pmb{f}(\pmb{z};\pmb{\theta}))
\end{align*}

이때
$$
\pmb{f}(\pmb{z};\pmb{\theta}) = 
\begin{bmatrix}
f(\pmb{z}_1; \pmb{\theta})\\
\vdots\\
f(\pmb{z}_n; \pmb{\theta})
\end{bmatrix}, \quad{}
\pmb{A}^{(k)}= 
\begin{bmatrix}
\nabla_{\pmb{\theta}}f(\pmb{z}_1,\theta_k)\\
\vdots\\
\nabla_{\pmb{\theta}}f(\pmb{z}_n,\theta_k)
\end{bmatrix}
$$

종합적으로 정리하면 뉴턴류의 방법은 미분계수가 필요하다.

## Other optimization methods

- **Secant methods**: Replace $J(\theta) = -\ell '' (\theta)$ by finite difference approximation

- **Fixed-point** methods ($\max_x g(x)$)
  + Find function $G(x)$ such that $G(x) = x \Longleftrightarrow g'(x) = 0$
  + Use updating scheme $x^{(t+1)} = G(x^{(t)})$
  + Obvious choice: $G(x) = \alpha g' (x) + x \Longrightarrow x^{(t+1)} = x^{(t)} + \alpha g'(x^{(t)})$
  + Requirements for convergence: (1) $x \in [a,b] \Longrightarrow G(x) \in [a,b]$, (2) $|G(x_1) - G(x_2) | \leq \lambda |x_1 - x_2|$ for all $x_1, x_2 \in [a,b]$ for some $\lambda \in (0,1)$
  
## Fixed Point

- 다음의 예를 보자.
$$
\text{argmax}_{x} g(x) = x\log (x) - x + 0.5x^2, \quad{} g'(x) = \log (x) + x
$$

- Possible choices of $G$:
$$
\begin{align*}
G_1 (x) &= g'(x) + x = \log (x) + 2x\\
G_2 (x) &= - \log (x)\\
G_3 (x) &= \exp (-x)\\
G_4 (x) &= (x+\exp(-x))/2
\end{align*}
$$


```{r}
#| echo: true
#| message: false
#| fig-align: center
#| fig-cap: "Figure: Fixed point example."
#| out-width: 70%
#| fig-height: 6
#| fig-weight: 6
#| view-distance: 10

#fixed_point_example.R
#https://www.uio.no/studier/emner/matnat/math/STK4051/v24/timeplan/lecture_01_chapter2_2024.pdf
#Fixed point
G = function(x){c(log(x[1])+2*x[1],-log(x[2]),exp(-x[3]),(x[4]+exp(-x[4]))/2)}

x = rep(10,4)
xM = matrix(x,ncol=4)
for(i in 1:20)
{
	x = c(G(x))
	xM = rbind(xM,x)
}
matplot(xM,type="l",ylim=c(-3,30),lty=1,lwd=2)
legend("topright",c("G1","G2","G3","G4"),lty=1,col=1:4)
xM

```

## Nelder-Mead

- Starts with $p+1$ distinct points $\pmb{x}_1, \ldots, \pmb{x}_{p+1}$

- Points ranked through $g(\pmb{x}_1), \ldots, g(\pmb{x}_{p+1})$

- $\pmb{x}_{\text{best}}$ and $\pmb{x}_{\text{worst}}$ best and worst points

- Calculate $\pmb{c} = \frac{1}{p}[\sum_{i=1}^{p+1}\pmb{x}_{i}-\pmb{x}_{\text{worst}}]$

- Find new value $\pmb{x}_r = \pmb{c} + \alpha (\pmb{c}- \pmb{x}_{\text{worst}})$, replace with $\pmb{x}_{\text{worst}}$

- Derivative를 필요로 하지 않는다는 장점

## Bisection

(교재 exercise 2.1)

- Start with two points one point at each side

- Propose a point in the middle

- Replace one endpoint

- Replace the one with the same sign of derivative as the one proposed

This is a case where we:

- Control error of approximation

- Error decay exponentially

- If $f'(x)$ is monotone between starting points on **each side**

## Gauss-Seidel

- Aim: maximize $g(\pmb{\theta})$, $\pmb{\theta} = (\theta_1, \ldots, \theta_p)$

- Procedure: For $j=1,\ldots, p$,
  + Maximize $g(\pmb{\theta})$ w.r.t. $\theta_j$ keeping the other $\theta_k$'s fixed
  + Reduce the multivariate problem to many univariate problems
  
## BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm

- Quasi-Newton (variable metric) method ($\text{argmax} g(\pmb{x})$)
$$
x_{k+1} = x_k - \alpha_k M_k^{-1}\nabla g(x_k)
$$

- $M_k$: an approximation to the Hessian

- $\alpha_k$: obtained by line-search

- Do a rank $1$ update of $M_k$ to $M_{k+1}$ using quantities computed during iterations

- Note: even though $x_k$ converges, $M_k$ may not converge to Hessian in optimum

## `optim` in R

```{r}
#| echo: true
#| message: false
#| eval: false
#| error: true
#| fig-align: center
#| fig-cap: "Figure: Fixed point example."
#| out-width: 70%
#| fig-height: 6
#| fig-weight: 6
#| view-distance: 10

optim(par, fn, gr=NULL,
method=c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
lower = -Inf, upper=Inf, control=list(), hessian=FALSE)

```

- Nelder-Mead: Default. Robust but can be slow

- BFGS:
  + $\pmb{x}^{(t+1)} = \pmb{x}^{(t)} - (\pmb{M}^{(t)})^{-1}\pmb{g}' (\pmb{x}^{(t)})$, $\pmb{M}^{(t)}$ approximation of $\pmb{g}''(\pmb{x}^{(t)})$
  + $\pmb{M}^{(t)}$: updated by a low-rank operation
  
- CG (conjugate gradient): Optimize along gradient direction (iteratively)

- L-BFGS-B: Modification of BFGS to allow for constraints

- SANN: Simulated annealing

- Brent: One-dimensional method

## Iteratively Reweighted Least Squares

### Weighted least square

- Assume you have data with unknown mean an known variance and want to estimate mean

\begin{align*}
\min_{\pmb{\beta}} \sum \frac{(y_i - \pmb{x}_i^T \pmb{\beta}_i)^2}{\sigma_i^2}&=\min_{\pmb{\beta}} \sum w_i (y_i -\pmb{x}_i^T)\\
\hat{\pmb{\beta}}_{\text{WLS}} = (\pmb{X}^T\pmb{W}\pmb{X})^{-1}\pmb{X}^T\pmb{Wy}
\end{align*}

이때 $\pmb{W} = \text{diag}(\pmb{w})$이다.

- 가중최소제곱의 유도

\begin{align*}
&\min_{\pmb{\beta}}\sum_{i=1}^n w_i (y_i -\pmb{\beta}^T x_i)^2\\
&\min_{\pmb{\beta}} (\pmb{y}-\pmb{X\beta})^T\pmb{W}(\pmb{y}-\pmb{X\beta})\\
&\frac{\partial}{\partial \pmb{\beta}}: 2\cdot \pmb{X}^T\pmb{W}(\pmb{y}-\pmb{X\beta})=0\\
&\pmb{X}^T\pmb{W}\pmb{X\beta} = \pmb{X}^T\pmb{Wy}\\
&\pmb{\beta}=(\pmb{X}^T\pmb{W}\pmb{X})^{-1}\pmb{X}^T\pmb{Wy}
\end{align*}

앞서는 분산을 알고있다고 하는 상황에서 $\hat{\pmb{\beta}}_{\text{WLS}}$를 구했다.

**Q**. 만약 분산이 알려져있지 않고 대신 모수의 함수라고 하면 어떻게 구할 것인가?
$$
\min_{\pmb{\beta}} \sum w_i (\pmb{\beta},\pmb{x}_i)(y_i - \pmb{x}_i^T\pmb{\beta}_i)^2
$$
Then update the weights with previous estimate of parameter.

- Useful trick, can be used for GLM (generalized linear models).

앞선 $\min_{\pmb{\beta}} \sum w_i (\pmb{\beta},\pmb{x}_i)(y_i - \pmb{x}_i^T\pmb{\beta}_i)^2$는 다음과 같이 반복법으로 풀 수 있다.

1. $\pmb{w}^{(0)}=\pmb{1}$

2. For $k=1$, until convergence

2-1. $\pmb{\beta}^{(k)} = \text{argmin}\Big\{ \sum w_i^{(k-1)}(y_i - \pmb{x}_i^T\pmb{\beta}_i)^2 \Big\},\quad{} \text{i.e. }\pmb{\beta}^{(k)}=\hat{\pmb{\beta}}_{\text{WLS}}(\pmb{W}^{(k-1)})$

2-2. $w_i^{(k)} = w_i (\pmb{\beta}^{(k)}, \pmb{x}_i)$

### L1 regression

- L1 regression은 outlier에 robust한 방법으로 quadratic loss를 absolute loss로 바꾸는 방법이다.

- Absolute loss:
$$
\text{argmin}_{\pmb{\beta}} \sum_{i=1}^n |y_i - \pmb{\beta}^T x_i|
$$
이때
$$
w_i (\pmb{\beta}) = \frac{1}{|y_i - \pmb{\beta}^T x_i|}
$$
로 두면
$$
\sum_{i=1}^n |y_i - \pmb{\beta}^T x_i| = \sum_{i=1}^n w_i (\pmb{\beta})(y_i - \pmb{\beta}^T x_i)^2
$$
여기서 IRLS를 하면 다음과 같다.

1. Start with $w_i^{(0)} = 1$ (=least squares regression) to get $\pmb{\beta}^{(0)}$

2. In iteration $k$ set $w_i^{(k)} = \frac{1}{|y_i -\pmb{\beta}^{(k-1)^T}x_i|}$ or $\min \Big\{\frac{1}{|y_i -\pmb{\beta}^{(k-1)^T}x_i|} , W_{\max} \Big\}$

```{r}
#| echo: true
#| message: false
#| eval: false
#| error: true
#| fig-align: center
#| fig-cap: "Figure: Fixed point example."
#| out-width: 70%
#| fig-height: 6
#| fig-weight: 6
#| view-distance: 10

#######################################################
## set upmodel
beta=c(1,1)
set.seed(231171)
n = 20

eps=rnorm(n,0,1)
x=rnorm(n,0,2)
Xdata=cbind(1,x)

y=beta[1]+beta[2]*x+eps

###############################################################
## code

## Least squares
leastSquares = function(X,y)
{
  betaHat = solve(t(X)%*%X) %*%(t(X)%*%y )
}


# IRLS

IRLS_L1 = function(X,y)
{  
  
  betaHat = solve(t(X)%*%X) %*%(t(X)%*%y )
  pred = X%*%betaHat
  res  =(y-pred)
  
  betaPrev=c(0,0)
  betaWHat=betaHat
  it=0
  
  while(sum(abs(betaPrev-betaWHat))>0.0001 & it<100)
  {  maxW=10
     it=it+1
     betaPrev=betaWHat
     pred= Xdata%*%betaPrev
     res=(y-pred)
     
     w = 1/abs(res)
     w[w>maxW]=maxW  # adjustment to avoid super large numbers [ size relative to problem]
     W = diag(as.vector(w))
     
     betaWHat = solve(t(Xdata)%*%W%*%Xdata) %*%(t(Xdata)%*%W%*%y )
     #show(betaWHat)
  }
  betaWHat
}

## Standard case 
betaHat_L2=leastSquares(Xdata,y)
betaHat_L1=IRLS_L1(Xdata,y)


Xplot=cbind(1,c(-5,0,5))

plot(x,y,xlim=c(-6,6),ylim=c(-6,6))
lines(c(-5,5),c(-4,6),lty=3)
lines(Xplot[,2], Xplot%*%betaHat_L2,lty = 2,col=3,lwd=2)
lines(Xplot[,2], Xplot%*%betaHat_L1,lty = 2,col=4,lwd=2)

maxW=10
pred= Xdata%*%betaHat_L1
res=(y-pred)
w = 1/abs(res)
plot(w,type="l",log="y",ylim=c(0.1,300))
w[w>maxW]=maxW  
points(w)


### Case with outliers 
ymod=y
ymod[1]=0
ymod[14]=0
betaHat_L2m=leastSquares(Xdata,ymod)
betaHat_L1m=IRLS_L1(Xdata,ymod)



plot(x,ymod,xlim=c(-6,6),ylim=c(-6,6))
lines(c(-5,5),c(-4,6),lty=3)
lines(Xplot[,2], Xplot%*%betaHat_L2m,lty = 2,col=3,lwd=2)
lines(Xplot[,2], Xplot%*%betaHat_L1m,lty = 2,col=4,lwd=2)



maxW=10
pred= Xdata%*%betaHat_L1m
res=(ymod-pred)
w = 1/abs(res)
plot(w,type="l",log="y",ylim=c(0.1,300))
w[w>maxW]=maxW  
points(w)

```

## Convex Optimization

Convex optimization은

- Least squares

- Linear programming

- Convex quadratic minimization with linear or convex quadratic constraints

- Conic optimization

- Second order cone programming

등 다양한 문제를 풀 수 있는 방법이다.

Covex optimization 문제를 다음과 같이 정의할 수 있다.

- $\text{Minimize}_{\pmb{x}}\quad{} \{f(\pmb{x})\}$ 
  + Subject to $\pmb{a}_i^T \pmb{x} = b_i$
  + and $g_j (\pmb{x})\leq 0$
  
- $i=1,\ldots, n_a$

- $j=1,\ldots, n_c$

- $f(\pmb{x}), g_j (\pmb{x})$: convex

이때 $f$는 convex 함수인데,
$$
f(t\pmb{x}_1 + (1-t)\pmb{x}_2) \leq t f(\pmb{x}_1) + (1-t)f(\pmb{x}_2), \quad{} \text{ for } 0 \leq t \leq 1
$$
을 의미한다.

### Lagrangian Multiplier

Convex optimization에서 $\pmb{A x} = \pmb{b}$와 같이 equality constraint가 있는 문제를 풀고 싶어 할 수 있다. 이 때 쓸 수 있는 방법이 **Lagrangian multiplier**이다.

\begin{align*}
&\text{minimize}_{\pmb{x}} \{ f(\pmb{x})\} &\text{subj to } \pmb{A x} = \pmb{b}\\
\Leftrightarrow& \text{minimize}_{\pmb{x}} \{ f(\pmb{x})+\frac{\rho}{2}\|\pmb{A x} - \pmb{b}\|^2 \}&\text{subj to } \pmb{A x} = \pmb{b}\\
\Leftrightarrow& \text{minimize}_{\pmb{x},\pmb{\lambda}} \{ f(\pmb{x})+\frac{\rho}{2}\|\pmb{A x} - \pmb{b}\|^2 +\pmb{\lambda}^T (\pmb{A x} - \pmb{b}) \}
\end{align*}

이떄

- $\pmb{x}: (p\times 1)$

- $\pmb{b}: (q \times 1)$

- $\pmb{A}: (q \times p)$

- $\rho: 1\times 1$ is a scalar

- $\lambda: (q \times 1)$: the Lagrangian multiplier

- Geometric interpretation: Minimum occurs when the contour line is tangent to constraint

```{r}
#| echo: false
#| message: false
#| fig-align: center
#| fig-cap: "Figure: Lagrangian multiplier."
#| out-width: 70%
#| fig-height: 6
#| fig-weight: 6
#| view-distance: 10

knitr::include_graphics("images/optim_LM.png")

```

그림은 Lagrangian multiplier를 설명하고 있는데, 그림에서 검은 선이 제약식을 나타낸다고 볼 수 있다. $f(\pmb{x})$를 $f(\pmb{x})+\frac{\rho}{2}\|\pmb{A x} - \pmb{b}\|^2$로 바꾸는 것은 최적화 결과에 영향을 미치지 않는다.

### Algorithm for solution: Method of multipliers

$\text{minimize}_{\pmb{x}} \{ f(\pmb{x})+\frac{\rho}{2}\|\pmb{A x} - \pmb{b}\|^2 \}\quad{}\text{subj to } \pmb{A x} = \pmb{b}$문제를 풀기 위해 **method of multipliers** 알고리즘을 적용할 수 있다.

1. $\pmb{\lambda}^{(0)} = \pmb{0}$

2. For $i=1$, until convergence

2-1. $\pmb{x}^{(i)}=\text{argmin}\Big\{ f(\pmb{x}) + \frac{\rho}{2}\|\pmb{A x} - \pmb{b}\|^2 + \pmb{\lambda}^{(i-1)^T}(\pmb{A x} - \pmb{b}) \Big\}$

2-2. $\pmb{\lambda}^{(i)} = \pmb{\lambda}^{(i-1)} + \rho (\pmb{A}\pmb{x}^{(i)}-\pmb{b})$

<!--
dual ascent라는 것도 찾아볼 것
-->

## Alternating Direction Method of Multipliers (ADMM)

최호식 외 2인 (2017) 에 따르면 [ADMM](https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf)은 원래의 문제보다 최적화가 쉬운 부분문제로 분할하고 이를 취합함으로써 복잡한 원 문제를 해결하는 방식의 근사알고리즘이다.

다음과 같이 앞서 다뤘던 문제보다 약간 더 복잡한 문제를 생각해보자.
$$
\begin{align*}
&\text{minimize} \{f(\pmb{x}) + g(\pmb{z}) \},\quad{} \text{subj to }\pmb{Ax} + \pmb{Bz}= \pmb{c}\\
\Longleftrightarrow&\text{minimize}\{ f(\pmb{x}) + g(\pmb{z}) + \pmb{\lambda}^{T}(\pmb{Ax} + \pmb{Bz}- \pmb{c}) +\frac{\rho}{2} \| \pmb{A x} + \pmb{Bz}- \pmb{c}\|^2 \}
\end{align*}
$$

$\pmb{\lambda}^{(0)}=\pmb{0}, \pmb{z}^{(0)}=\pmb{0}$

For $i=1, \ldots$, until convergence

1. $\pmb{x}^{(i)} = \text{argmin}\Big\{ f(\pmb{x}) + \frac{\rho}{2}\| \pmb{A x} + \pmb{B z}^{(i-1)} - \pmb{c}\|^2 + \pmb{\lambda}^{(i-1)^T}(\pmb{A x} + \pmb{B z}^{(i-1)} - \pmb{c}) \Big\}$

2. $\pmb{z}^{(i)} = \text{argmin}\Big\{ g(\pmb{z}) + \frac{\rho}{2}\| \pmb{A x}^{(i)} + \pmb{B z} - \pmb{c}\|^2 + \pmb{\lambda}^{(i-1)^T}(\pmb{A x}^{(i)}  + \pmb{B z}- \pmb{c}) \Big\}$

3. $\pmb{\lambda}^{(i)}=\pmb{\lambda}^{(i-1)}+\rho (\pmb{A x}^{(i)}  + \pmb{B z}^{(i)}- \pmb{c}) $

정리해보면, $f(\pmb{x}), g(\pmb{z})$를 동시에 다룰 필요 없이 먼저 $\pmb{x}$에 대해 풀고, 그 다음 $\pmb{z}$에 대해 풀고, 마지막으로 $\pmb{\lambda}$를 업데이트 하면 된다는 것이다.

### ADMM을 이용해 LASSO 풀기

- LASSO는 다음을 푸는 것이다.
$$
\begin{align*}
&\text{minimize} \Big\{\frac{1}{2}\| \pmb{X \beta} -\pmb{y}\|^2 +\gamma \|\pmb{\beta}\|_1 \Big\}\\
\Longleftrightarrow&\text{minimize}\Big\{\frac{1}{2}\| \pmb{X \beta} -\pmb{y}\|^2 +\gamma \|\pmb{z}\|_1 \Big\} \quad{} \text{subj to }\pmb{\beta} - \pmb{z} = \pmb{0}
\end{align*}
$$